# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uPpoFveQn6ZxIFJDP7k4unzTe--_Gj1J
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn import metrics
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import RandomOverSampler

import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('asd_data_csv.csv')
df.head()

df.shape

df.info()

# separating the data and labels
X= df.drop(columns = 'Outcome', axis=1)
Y= df['Outcome']

from sklearn.feature_selection import mutual_info_classif
info_gains = mutual_info_classif(X,Y)

info_dict = {f'Feature {i+1}': gain for i, gain in enumerate(info_gains)}

# Sort features by information gain in descending order
sorted_info = {k: v for k, v in sorted(info_dict.items(), key=lambda item: item[1], reverse=True)}

# Display features and their information gains in descending order
for feature, gain in sorted_info.items():
    print(f"{feature}: {gain}")



# Threshold for selecting features
threshold = 0.0417

# Get indices of features above the threshold
selected_features = np.where(info_gains >= threshold)[0]

# Filter the features based on the threshold
X_selected = X[:, selected_features]

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X_selected, Y, test_size=0.2, random_state=42)

# Normalizing the features for stable and fast training.
scaler = StandardScaler()
X = scaler.fit_transform(X)
X_val = scaler.transform(X_val)

from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report

# Assuming X, Y, X_val, Y_val are defined

models = [LogisticRegression(), XGBClassifier(), SVC(kernel='rbf')]

for model in models:
    model.fit(X, Y)  # Train the model

    # Evaluate on training and validation sets
    train_predictions = model.predict(X)
    val_predictions = model.predict(X_val)

    # ROC AUC score for training set
    train_auc = roc_auc_score(Y, train_predictions)
    # ROC AUC score for validation set
    val_auc = roc_auc_score(Y_val, val_predictions)

    print(f'{model} : ')
    print('Training ROC AUC: ', train_auc)
    print('Validation ROC AUC: ', val_auc)
    print()

    # Additional evaluation metrics - accuracy and classification report
    accuracy = accuracy_score(Y_val, val_predictions)
    print("Accuracy:", accuracy)
    print("\nClassification Report:")
    print(classification_report(Y_val, val_predictions))
    print()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming models[1] is the model you want to evaluate
model = models[1]

# Get predictions on the validation set
predictions = model.predict(X_val)

# Generate confusion matrix
cm = confusion_matrix(Y_val, predictions)

# Plot confusion matrix using seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

input_data = (8,9,1,1,1,1,1,1,2,1,0,1,1)
#input_data = (7,3,1,1,1,1,1,1,4,1,0,1,0)
# changing the input_data to numpy array
input_data_as_numpy_array = np.asarray(input_data)

# reshape the array as we are predicting for one instance
input_data_reshaped = input_data_as_numpy_array.reshape(1,-1)

# standardize the input data
std_data = scaler.transform(input_data_reshaped)
print(std_data)

prediction = models[1].predict(std_data)
print(prediction)

if (prediction[0] == 0):
  print('The person is not with Autism spectrum disorder')
else:
  print('The person is with Autism spectrum disorder')